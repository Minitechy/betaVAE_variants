{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548f5c85-34f4-4bb2-a299-982a1bbae698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f002a265-8e02-49bf-a0e5-8ff67ada0c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom dataset class for dSprites dataset\n",
    "class DspritesDataset(Dataset):\n",
    "    def __init__(self, npz_file):\n",
    "        \"\"\"\n",
    "        Initializes the dSprites dataset by loading the image data and ground truth factors.\n",
    "\n",
    "        Args:\n",
    "            npz_file (str): Path to the .npz file containing the dataset.\n",
    "        \"\"\"\n",
    "        self.data = np.load(npz_file)\n",
    "        self.images = np.expand_dims(self.data['imgs'], axis=1)  # Add channel dimension for CNN\n",
    "        self.latent_classes = self.data['latents_classes'][:, 1:]  # Skip the 'color' factor (always 1)\n",
    "        self.latent_values = self.data['latents_values'][:, 1:]  # Skip the 'color' factor (always 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of images.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches an image and its corresponding ground truth factors.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Image as torch.Tensor, latent classes, and latent values.\n",
    "        \"\"\"\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32)  # Convert image to float32 tensor\n",
    "        latent_classes = torch.tensor(self.latent_classes[idx], dtype=torch.int)  # Ground truth factors (e.g., shape, scale)\n",
    "        latent_values = torch.tensor(self.latent_values[idx], dtype=torch.float32)  # Continuous values for factors\n",
    "\n",
    "        return image, latent_classes, latent_values\n",
    "\n",
    "# Encoder architecture\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        Encoder for beta-VAE using convolutional layers.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Fully connected layers for mu and logvar\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Fully connected layer with 256 units\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)  # Latent mean\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)  # Latent log-variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder to produce latent space parameters.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            mu (torch.Tensor): Mean of the latent space.\n",
    "            logvar (torch.Tensor): Log-variance of the latent space.\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)  # Flatten the output of the convolution layers\n",
    "        h = self.fc1(h)  # Apply fully connected layer\n",
    "        mu = self.fc_mu(h)  # Mean\n",
    "        logvar = self.fc_logvar(h)  # Log variance\n",
    "        return mu, logvar\n",
    "\n",
    "# Decoder architecture\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_channels=1):\n",
    "        \"\"\"\n",
    "        Decoder for beta-VAE using transpose convolutional layers.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "            num_channels (int): Number of output image channels (1 for grayscale images).\n",
    "        \"\"\"\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)  # Latent dim -> 256\n",
    "        self.fc2 = nn.Linear(256, 64 * 4 * 4)  # 256 -> 4x4x64\n",
    "        \n",
    "        # Transpose convolutional layers with adjusted padding\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, num_channels, kernel_size=4, stride=2, padding=1),  # 32x32 -> 64x64\n",
    "            nn.Sigmoid()  # Sigmoid activation to output pixel values in range [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder to reconstruct the image from the latent space.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent vector.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed image.\n",
    "        \"\"\"\n",
    "        h = self.fc1(z)\n",
    "        h = self.fc2(h)\n",
    "        h = h.view(h.size(0), 64, 4, 4)  # Reshape to match convolutional layers\n",
    "        return self.decoder(h)\n",
    "\n",
    "# beta-VAE model\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        beta-VAE model with encoder and decoder networks.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from a Gaussian distribution.\n",
    "\n",
    "        Args:\n",
    "            mu (torch.Tensor): Mean of the latent space.\n",
    "            logvar (torch.Tensor): Log-variance of the latent space.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reparameterized latent vector.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)  # Random normal noise\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire VAE model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed image.\n",
    "            mu (torch.Tensor): Mean of the latent space.\n",
    "            logvar (torch.Tensor): Log-variance of the latent space.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ce69ae-b1e4-4152-849b-fd3a43058a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_beta_vae_loss(recon_x, x, mu, logvar, beta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Computes the lambda-beta-VAE loss, which includes:\n",
    "    1. Reconstruction error (BCE)\n",
    "    2. KL divergence (KLD)\n",
    "    3. L2 norm regularization on the difference between original and reconstructed images.\n",
    "\n",
    "    Args:\n",
    "        recon_x (torch.Tensor): Reconstructed image.\n",
    "        x (torch.Tensor): Original image.\n",
    "        mu (torch.Tensor): Latent mean from the encoder.\n",
    "        logvar (torch.Tensor): Latent log-variance from the encoder.\n",
    "        beta (float): Scaling factor for KL divergence term.\n",
    "        lambda_reg (float): Regularization factor for the L2 norm term.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: BCE loss, KL divergence loss, L2 norm loss, and total loss.\n",
    "    \"\"\"\n",
    "    # Binary cross-entropy loss (sum reduction)\n",
    "    bce_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence loss for latent space regularization\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # L2 norm regularization on the difference between original and reconstructed images\n",
    "    l2_norm_loss = torch.sum((recon_x - x).pow(2))\n",
    "\n",
    "    # Total loss = Reconstruction error + beta * KL divergence + L2 norm\n",
    "    total_loss = bce_loss + beta * kl_loss + lambda_reg * l2_norm_loss\n",
    "\n",
    "    return bce_loss, kl_loss, l2_norm_loss, total_loss\n",
    "\n",
    "# Training function for beta-VAE with an additional lambda term for L2 regularization on reconstruction\n",
    "def train_lambda_beta_vae(model, train_loader, val_loader, optimizer, loss_fn, beta, lambda_reg, device, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the lambda-beta-VAE model and tracks losses for both training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The lambda-beta-VAE model.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        val_loader (DataLoader): DataLoader for the validation set.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the training process.\n",
    "        loss_fn (function): Loss function to compute lambda-beta-VAE loss.\n",
    "        beta (float): Scaling factor for KL divergence term.\n",
    "        lambda_reg (float): Regularization factor for the L2 norm term.\n",
    "        device (torch.device): Device to run the model on (CPU or GPU).\n",
    "        num_epochs (int): Number of epochs for training.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Lists containing the history of total loss, KL divergence, BCE loss, and L2 loss for both training and validation.\n",
    "    \"\"\"\n",
    "    # Lists to store loss history\n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    kl_loss_hist, val_kl_loss_hist = [], []\n",
    "    bce_loss_hist, val_bce_loss_hist = [], []  # BCE loss (unregularized)\n",
    "    l2_loss_hist, val_l2_loss_hist = [], []  # Separate L2 loss history\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, kl_loss, bce_loss, l2_loss = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        # Training step\n",
    "        for batch, _, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            bce_loss_batch, kl_loss_batch, l2_loss_batch, total_loss_batch = loss_fn(recon_batch, batch, mu, logvar, beta, lambda_reg)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            train_loss += total_loss_batch.item()\n",
    "            kl_loss += kl_loss_batch.item()\n",
    "            bce_loss += bce_loss_batch.item()  # Accumulate BCE loss separately\n",
    "            l2_loss += l2_loss_batch.item()  # Accumulate L2 loss\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss, val_kl_loss, val_bce_loss, val_l2_loss = 0.0, 0.0, 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch, _, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation'):\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                recon_batch, mu, logvar = model(batch)\n",
    "\n",
    "                # Compute loss\n",
    "                bce_loss_batch, kl_loss_batch, l2_loss_batch, total_loss_batch = loss_fn(recon_batch, batch, mu, logvar, beta, lambda_reg)\n",
    "\n",
    "                # Accumulate validation losses\n",
    "                val_loss += total_loss_batch.item()\n",
    "                val_kl_loss += kl_loss_batch.item()\n",
    "                val_bce_loss += bce_loss_batch.item()  # Accumulate BCE loss separately for validation\n",
    "                val_l2_loss += l2_loss_batch.item()  # Accumulate L2 loss for validation\n",
    "\n",
    "        # Average losses over the entire dataset\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_kl_loss = kl_loss / len(train_loader.dataset)\n",
    "        avg_bce_loss = bce_loss / len(train_loader.dataset)  # Average BCE loss for training\n",
    "        avg_l2_loss = l2_loss / len(train_loader.dataset)  # Average L2 loss for training\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        avg_val_kl_loss = val_kl_loss / len(val_loader.dataset)\n",
    "        avg_val_bce_loss = val_bce_loss / len(val_loader.dataset)  # Average BCE loss for validation\n",
    "        avg_val_l2_loss = val_l2_loss / len(val_loader.dataset)  # Average L2 loss for validation\n",
    "\n",
    "        # Logging the losses\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train BCE: {avg_bce_loss:.4f}, Train KL: {avg_kl_loss:.4f}, Train L2 Norm: {avg_l2_loss:.4f}, Train Total Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Val BCE: {avg_val_bce_loss:.4f}, Val KL: {avg_val_kl_loss:.4f}, Val L2 Norm: {avg_val_l2_loss:.4f}, Val Total Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Storing the losses in history\n",
    "        train_loss_hist.append(avg_train_loss)\n",
    "        kl_loss_hist.append(avg_kl_loss)\n",
    "        bce_loss_hist.append(avg_bce_loss)  # Track BCE loss\n",
    "        l2_loss_hist.append(avg_l2_loss)  # Track training L2 loss\n",
    "        \n",
    "        val_loss_hist.append(avg_val_loss)\n",
    "        val_kl_loss_hist.append(avg_val_kl_loss)\n",
    "        val_bce_loss_hist.append(avg_val_bce_loss)  # Track BCE loss for validation\n",
    "        val_l2_loss_hist.append(avg_val_l2_loss)  # Track validation L2 loss\n",
    "\n",
    "    return train_loss_hist, val_loss_hist, kl_loss_hist, val_kl_loss_hist, bce_loss_hist, val_bce_loss_hist, l2_loss_hist, val_l2_loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8bc48e-27af-4e36-8177-704f288dff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select exactly 3 distinct shape indices\n",
    "def select_3_distinct_shape_indices(dataset, split_indices):\n",
    "    \"\"\"\n",
    "    Selects one index for each distinct shape (square, ellipse, heart) from the provided split\n",
    "    and prints the count of images for each shape.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset containing images and latent classes.\n",
    "    - split_indices: The indices corresponding to the data split (train, validation, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - distinct_shape_indices: A list containing one randomly selected index for each distinct shape.\n",
    "    \"\"\"\n",
    "    # Dictionary to store indices for each shape\n",
    "    shape_indices = {0: [], 1: [], 2: []}\n",
    "    \n",
    "    # Iterate over the split indices to collect indices for each shape\n",
    "    for idx in split_indices:\n",
    "        _, latent_classes, _ = dataset[idx]\n",
    "        shape_class = latent_classes[0].item()  # Shape class: 0 = square, 1 = ellipse, 2 = heart\n",
    "        \n",
    "        # Add the index to the corresponding shape list\n",
    "        shape_indices[shape_class].append(idx)\n",
    "    \n",
    "    # Print the number of images for each shape\n",
    "    shape_names = {0: 'square', 1: 'ellipse', 2: 'heart'}\n",
    "    for shape_class, indices in shape_indices.items():\n",
    "        print(f\"Number of {shape_names[shape_class]} images: {len(indices)}\")\n",
    "    \n",
    "    # Randomly select one index from each shape class\n",
    "    distinct_shape_indices = [random.choice(shape_indices[shape]) for shape in range(3)]\n",
    "    \n",
    "    return distinct_shape_indices\n",
    "\n",
    "def visualize_3_images_from_shapes(dataset, distinct_shape_indices):\n",
    "    \"\"\"\n",
    "    Visualize 3 images corresponding to distinct shapes (square, ellipse, heart) and save them.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset containing images and latent classes.\n",
    "    - distinct_shape_indices: A list of 3 indices corresponding to distinct shape images.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If distinct_shape_indices does not contain exactly 3 indices.\n",
    "    \"\"\"\n",
    "    # Ensure exactly 3 indices are provided\n",
    "    if len(distinct_shape_indices) != 3:\n",
    "        raise ValueError(\"distinct_shape_indices must contain exactly 3 indices.\")\n",
    "    \n",
    "    # Define the shape names to label and save the images\n",
    "    shape_names = ['square', 'ellipse', 'heart']\n",
    "    \n",
    "    # Create a figure with 3 subplots (1 row, 3 columns)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    # Iterate through the indices, display, and save the images\n",
    "    for i, (idx, shape_name) in enumerate(zip(distinct_shape_indices, shape_names)):\n",
    "        image, _, _ = dataset[idx]  # Retrieve the image at the specified index\n",
    "        \n",
    "        # Display the image in the subplot\n",
    "        axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"{shape_name}\", fontsize=16)\n",
    "\n",
    "        # Save each image with its corresponding shape name\n",
    "        image_save_path = os.path.join(SAVE_PATH, f\"{shape_name}.png\")\n",
    "        plt.imsave(image_save_path, image.squeeze(), cmap='gray')\n",
    "\n",
    "    # Adjust layout and show the figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e708e3-e8e9-4d68-9623-f97c2cd42942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot losses for both training and validation over epochs\n",
    "def plot_losses(train_loss_hist, val_loss_hist, kl_loss_hist, val_kl_loss_hist, recon_loss_hist, val_recon_loss_hist, l2_loss_hist, val_l2_loss_hist, beta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Plot the total loss, KL divergence, reconstruction error, and L2 loss for both training and validation sets.\n",
    "\n",
    "    Args:\n",
    "    - train_loss_hist (list): History of training total losses (-ELBO).\n",
    "    - val_loss_hist (list): History of validation total losses (-ELBO).\n",
    "    - kl_loss_hist (list): History of training KL divergence losses.\n",
    "    - val_kl_loss_hist (list): History of validation KL divergence losses.\n",
    "    - recon_loss_hist (list): History of training reconstruction errors.\n",
    "    - val_recon_loss_hist (list): History of validation reconstruction errors.\n",
    "    - l2_loss_hist (list): History of training L2 norm losses.\n",
    "    - val_l2_loss_hist (list): History of validation L2 norm losses.\n",
    "    - beta (float): The beta value used for the loss scaling in beta-VAE.\n",
    "    - lambda_reg (float): The lambda regularization factor.\n",
    "    \"\"\"\n",
    "    epochs = len(train_loss_hist)\n",
    "\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(20, 6))\n",
    "\n",
    "    # Subplot for Total Loss (-ELBO)\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(range(epochs), train_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Total Loss', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Subplot for KL Divergence\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(range(epochs), kl_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_kl_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('KL Divergence', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Subplot for Reconstruction Error\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(range(epochs), recon_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_recon_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('BCE Loss', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Subplot for L2 Loss\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.plot(range(epochs), l2_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_l2_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('L2 Norm Loss', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1, wspace=0.3)\n",
    "\n",
    "    # Save and display the plot\n",
    "    plt.savefig(f\"{SAVE_PATH}loss_plots_lambda_{lambda_reg}_beta_{beta}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccf43b6-e41a-4ea8-b741-15859fdf0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of original and reconstructed images in separate grids\n",
    "def visualize_images_grid(model, data_loader, device, beta, batch_size, lambda_reg):\n",
    "    \"\"\"\n",
    "    Visualize and save original and reconstructed images from a batch of data.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Trained Beta-VAE model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - beta (float): The beta value used for the loss scaling in beta-VAE.\n",
    "    - batch_size (int): Number of images to visualize.\n",
    "    - save_path (str): Directory to save the output images.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Retrieve one batch of images from the data loader\n",
    "    images, _, _ = next(iter(data_loader))\n",
    "    images = images[:batch_size]  # Limit to the specified batch size\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)  # Move images to the specified device\n",
    "        mu, logvar = model.encoder(images)  # Obtain the latent mean (mu) and log variance (logvar) from the encoder\n",
    "        \n",
    "        # Sample z from the latent distribution using the reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)  # Calculate the standard deviation\n",
    "        epsilon = torch.randn_like(std)  # Sample epsilon from a standard normal distribution\n",
    "        z = mu + std * epsilon  # Reparameterization trick\n",
    "\n",
    "        recon_images = model.decoder(z)  # Decode to get the reconstructed images\n",
    "\n",
    "    img_dim = images[0].cpu().squeeze().shape[0]  # Assuming square images (e.g., 64x64)\n",
    "\n",
    "    # Create separate images for original and reconstructed images\n",
    "    original_image_grid = np.ones((8 * img_dim, 8 * img_dim))  # 8 rows, 8 columns grid\n",
    "    reconstructed_image_grid = np.ones((8 * img_dim, 8 * img_dim))  # 8 rows, 8 columns grid\n",
    "\n",
    "    # Fill in the grids with images\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            idx = i * 8 + j\n",
    "            if idx < batch_size:\n",
    "                # Fill the original image grid\n",
    "                original_image_grid[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim] = images[idx].cpu().squeeze()\n",
    "\n",
    "                # Fill the reconstructed image grid\n",
    "                reconstructed_image_grid[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim] = recon_images[idx].cpu().squeeze()\n",
    "\n",
    "    # Save the original images only for the first beta = 1\n",
    "    if beta == 1:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(original_image_grid, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{SAVE_PATH}/original_images.png\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    # Save the reconstructed images for all betas\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(reconstructed_image_grid, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{SAVE_PATH}/reconstructed_images_lambda_{lambda_reg}_beta_{beta}.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "def visualize_random_decoder_outputs(model, device, latent_dim, beta, lambda_reg, num_images=64):\n",
    "    \"\"\"\n",
    "    Visualize a grid of images generated by the decoder from random latent vectors.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Trained Beta-VAE model.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - latent_dim (int): Number of latent dimensions in the VAE.\n",
    "    - num_images (int): Number of images to generate and visualize.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Sample random latent vectors from a standard normal distribution (mu=0, std=1)\n",
    "        z = torch.randn(num_images, latent_dim).to(device)\n",
    "\n",
    "        # Generate images using the decoder\n",
    "        generated_images = model.decoder(z).cpu().numpy()\n",
    "\n",
    "    # Define image and grid parameters\n",
    "    img_dim = generated_images.shape[2]  # Assuming square images (e.g., 64x64)\n",
    "    grid_size = int(np.sqrt(num_images))  # Arrange images in a square grid\n",
    "\n",
    "    # Create a blank canvas to hold the generated images\n",
    "    combined_image = np.ones((grid_size * img_dim, grid_size * img_dim))\n",
    "\n",
    "    # Fill the canvas with generated images\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            idx = i * grid_size + j\n",
    "            combined_image[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim] = generated_images[idx].squeeze()\n",
    "\n",
    "    # Plot the grid of generated images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(combined_image, cmap='gray')\n",
    "    plt.axis('off')  # Remove axis for cleaner visualization\n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()  # Get the current figure object\n",
    "    fig.savefig(f\"{SAVE_PATH}/decoder_images_lambda_{lambda_reg}_beta_{beta}.png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c24d781-61c5-4625-af82-63b7f91b680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_traversal_combined(model, device, gif_path, distinct_shape_indices, dataset, latent_dim=10, num_traversals=11):\n",
    "    \"\"\"\n",
    "    Visualize latent space traversal for dSprites dataset, creating both PNG and GIF outputs.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): Trained Beta-VAE model.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - gif_path (str): Path to save the generated GIF and PNG files.\n",
    "    - distinct_shape_indices (list): List of indices representing distinct shapes in the dataset.\n",
    "    - dataset (Dataset): dSprites dataset object.\n",
    "    - latent_dim (int): The number of latent dimensions (default=10).\n",
    "    - num_traversals (int): Number of traversal steps between -2 and 2 (default=11).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Define the interpolation range (from -2 to 2)\n",
    "    interpolation = torch.linspace(-2, 2, num_traversals)\n",
    "\n",
    "    # Prepare for storing frames for the final combined GIF (3 rows x num_traversals columns)\n",
    "    combined_gif_frames_all_shapes = [[] for _ in range(num_traversals)]\n",
    "\n",
    "    # Loop through each distinct shape (3 rows in the final GIF)\n",
    "    for row, idx in enumerate(distinct_shape_indices):\n",
    "        image, _, _ = dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        # Get latent mean (mu) and log-variance (logvar) from the encoder\n",
    "        mu, logvar = model.encoder(image)\n",
    "\n",
    "        # Compute standard deviation (std) from logvar\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        # Sample epsilon (eps) from a standard normal distribution\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        # Use the reparameterization trick: z = mu + eps * std\n",
    "        z = mu + eps * std      \n",
    "\n",
    "        # Prepare for storing frames for the current shape's latent traversal\n",
    "        combined_gif_frames = [[] for _ in range(num_traversals)]\n",
    "\n",
    "        # Perform traversal for each latent dimension\n",
    "        for latent_idx in range(latent_dim):\n",
    "            z_ori = z.clone()  # Clone the latent vector\n",
    "\n",
    "            # Perform interpolation across the specified latent dimension\n",
    "            for i, alpha in enumerate(interpolation):\n",
    "                z_copy = z_ori.clone()\n",
    "                z_copy[:, latent_idx] = alpha  # Set this latent dimension to the interpolation value\n",
    "\n",
    "                # Generate the reconstructed image using the decoder\n",
    "                with torch.no_grad():\n",
    "                    generated_image = model.decoder(z_copy)\n",
    "\n",
    "                # Convert to numpy array and append the frame for saving\n",
    "                gen_img = generated_image.squeeze().cpu().numpy()\n",
    "                combined_gif_frames[i].append(np.uint8(255 * gen_img))\n",
    "\n",
    "        # Create grid for latent traversals (one shape, 10 latent dimensions, 11 traversal steps)\n",
    "        gif_frames_with_10_columns = [np.hstack(frame_list) for frame_list in combined_gif_frames]\n",
    "\n",
    "        # Store frames for all shapes into the combined GIF structure\n",
    "        for i in range(num_traversals):\n",
    "            if row == 0:\n",
    "                combined_gif_frames_all_shapes[i] = [gif_frames_with_10_columns[i]]\n",
    "            else:\n",
    "                combined_gif_frames_all_shapes[i].append(gif_frames_with_10_columns[i])\n",
    "\n",
    "        # Generate and save PNG for the current shape's latent traversals\n",
    "        png_output_path = f\"{gif_path}_image_{row + 1}_traversal.png\"\n",
    "        save_png_latent_traversal_only(combined_gif_frames, png_output_path, latent_dim, num_traversals)\n",
    "\n",
    "    # Combine frames of all shapes for the final GIF (3 rows x num_traversals columns)\n",
    "    final_gif_frames = [np.vstack(frame_list) for frame_list in combined_gif_frames_all_shapes]\n",
    "\n",
    "    # Save the final GIF combining all shapes and latent traversals\n",
    "    gif_output_path = f\"{gif_path}_final_combined.gif\"\n",
    "    save_combined_gif(final_gif_frames, gif_output_path)\n",
    "\n",
    "# Helper function to save the final combined GIF (3 rows and 10 columns) with column titles\n",
    "def save_combined_gif(frames, output_path):\n",
    "    \"\"\"\n",
    "    Save the combined GIF that shows the latent traversal for multiple shapes.\n",
    "\n",
    "    Parameters:\n",
    "    - frames (list): List of frames to include in the GIF.\n",
    "    - output_path (str): Path to save the GIF.\n",
    "    \"\"\"\n",
    "    gif_frames_with_labels = []\n",
    "\n",
    "    for frame in frames:\n",
    "        if frame is not None and frame.size > 0:\n",
    "            fig, ax = plt.subplots(figsize=(20, 6))\n",
    "            ax.imshow(frame, cmap='gray', vmin=0, vmax=255)  # Display the frame in grayscale\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add column labels (x_1 to x_10) for each latent dimension\n",
    "            column_labels = [f'$x_{{{i + 1}}}$' for i in range(10)]\n",
    "            for i, label in enumerate(column_labels):\n",
    "                x_pos = (frame.shape[1] / 10) * (i + 0.5)  # Calculate label position\n",
    "                ax.text(x_pos, -10, label, fontsize=14, ha='center', va='bottom', color='black')\n",
    "\n",
    "            # Ensure the figure is drawn before converting to an image\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            # Convert the figure to an image array and append it to the list of frames\n",
    "            gif_frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8).reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "            gif_frames_with_labels.append(gif_frame)\n",
    "            plt.close(fig)\n",
    "\n",
    "    # Save the GIF using imageio\n",
    "    if gif_frames_with_labels:\n",
    "        # Convert images to uint8\n",
    "        gif_frames_with_labels = [np.uint8(frame) for frame in gif_frames_with_labels]\n",
    "        imageio.mimsave(output_path, gif_frames_with_labels, fps=5, loop=0)  # loop=0 for infinite looping\n",
    "    else:\n",
    "        print(\"No valid frames to save for GIF.\")\n",
    "\n",
    "# Function to save PNG for latent traversals with row labels for each latent dimension\n",
    "def save_png_latent_traversal_only(traversal_images, output_path, latent_dim=10, num_traversals=11):\n",
    "    \"\"\"\n",
    "    Save a PNG file for visualizing the latent traversals across all dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - traversal_images (list): List of images generated during the traversal.\n",
    "    - output_path (str): Path to save the PNG.\n",
    "    - latent_dim (int): Number of latent dimensions.\n",
    "    - num_traversals (int): Number of interpolation steps.\n",
    "    \"\"\"\n",
    "    # Create a figure with tight-fitting subplots (no spacing between images)\n",
    "    fig, axes = plt.subplots(latent_dim, num_traversals, figsize=(num_traversals * 2, latent_dim * 2))\n",
    "\n",
    "    # Define the interpolation values for the column labels from -2 to 2\n",
    "    interpolation_values = np.linspace(-2, 2, num_traversals)\n",
    "\n",
    "    # Plot the traversal images in a grid for all latent dimensions\n",
    "    for latent_idx in range(latent_dim):\n",
    "        for frame_idx in range(num_traversals):\n",
    "            axes[latent_idx, frame_idx].imshow(traversal_images[frame_idx][latent_idx], cmap='gray', aspect='auto')\n",
    "            axes[latent_idx, frame_idx].axis('off')  # Hide axis\n",
    "\n",
    "    # Add row labels for each latent dimension (x_1 through x_10)\n",
    "    row_labels = [f'$x_{{{i + 1}}}$' for i in range(latent_dim)]\n",
    "    for i, label in enumerate(row_labels):\n",
    "        axes[i, 0].annotate(label, xy=(-0.05, 0.5), xycoords='axes fraction', xytext=(-30, 0), textcoords='offset points',\n",
    "                            ha='center', va='center', fontsize=18, color='black')\n",
    "\n",
    "    # Add column labels for x-axis (-2 to +2)\n",
    "    for frame_idx in range(num_traversals):\n",
    "        axes[0, frame_idx].set_title(f'{interpolation_values[frame_idx]:.1f}', fontsize=18, color='black')\n",
    "\n",
    "    # Adjust layout to remove spacing between images\n",
    "    plt.subplots_adjust(left=0.05, right=0.99, top=0.9, bottom=0.1, wspace=0, hspace=0)\n",
    "\n",
    "    # Save the PNG\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fced51a9-1ec3-4589-8116-fd97a24d67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract latent variables, ground truth factors, and latent statistics (mu and std) from the model\n",
    "def extract_latents_and_factors(model, data_loader, device, flag):\n",
    "    \"\"\"\n",
    "    Extract latent representations (sampled from Gaussian distribution), corresponding ground truth factors, \n",
    "        and latent statistics from the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): Trained Beta-VAE model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader object to load the dataset.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - flag (int): 0 for training data, 1 for validation data (used for setting description).\n",
    "    \n",
    "    Returns:\n",
    "    - latents (numpy.ndarray): Sampled latent representations of the data.\n",
    "    - latent_classes (numpy.ndarray): Discrete ground truth factors for the data.\n",
    "    - latent_values (numpy.ndarray): Continuous ground truth factors for the data.\n",
    "    - mu_list (numpy.ndarray): List of mean values for the latent variables.\n",
    "    - std_list (numpy.ndarray): List of standard deviations for the latent variables.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    latents, latent_classes_list, latent_values_list = [], [], []\n",
    "    mu_list, std_list = [], []\n",
    "\n",
    "    # Set the description for the progress bar based on the flag (training or validation)\n",
    "    desc = 'Training' if flag == 0 else 'Validation'\n",
    "    \n",
    "    # No gradient computation needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the DataLoader to extract latent variables and ground truth factors\n",
    "        for image, latent_classes, latent_values in tqdm(data_loader, desc=desc):\n",
    "            image = image.to(device)  # Move input images to the target device\n",
    "            \n",
    "            # Get the latent mean (mu) and log variance (logvar) from the encoder\n",
    "            mu, logvar = model.encoder(image)\n",
    "            \n",
    "            # Compute standard deviation from log variance\n",
    "            sigma = torch.exp(0.5 * logvar)\n",
    "            \n",
    "            # Sample from the Gaussian distribution (reparameterization trick)\n",
    "            epsilon = torch.randn_like(sigma)\n",
    "            z = mu + sigma * epsilon  # Sampled latent variable\n",
    "            \n",
    "            # Store the sampled latent representations, corresponding factors, and statistics\n",
    "            latents.append(z.cpu())\n",
    "            latent_classes_list.append(latent_classes)\n",
    "            latent_values_list.append(latent_values)\n",
    "            mu_list.append(mu.cpu())\n",
    "            std_list.append(sigma.cpu())\n",
    "\n",
    "    # Concatenate lists to form the final latent representations and ground truth arrays\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "    latent_classes = torch.cat(latent_classes_list, dim=0).numpy()\n",
    "    latent_values = torch.cat(latent_values_list, dim=0).numpy()\n",
    "    mu_list = torch.cat(mu_list, dim=0).numpy()\n",
    "    std_list = torch.cat(std_list, dim=0).numpy()\n",
    "    \n",
    "    return latents, latent_classes, latent_values, mu_list, std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5547077a-6667-474c-a625-ae1143baa216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to train and evaluate the beta-VAE model with lambda and beta\n",
    "def main():\n",
    "    # Loop through each beta, then loop over lambda\n",
    "    for beta in BETAS:\n",
    "        print(colored(f\"\\nProcessing with beta = {beta}...\", \"green\", attrs=['bold']))\n",
    "        for lambda_reg in LAMBDAS:\n",
    "            # Define model and check if pre-trained weights exist\n",
    "            model_save_path = os.path.join(SAVE_PATH, f'beta_vae_lambda_{lambda_reg}_beta_{beta}.pth')\n",
    "            model = BetaVAE(latent_dim=LATENT_DIM).to(DEVICE)\n",
    "            trained = False  # Flag to indicate whether the model was trained\n",
    "\n",
    "            if os.path.exists(model_save_path):\n",
    "                print(colored(f\"Loading saved model weights for beta = {beta}, lambda = {lambda_reg}...\", \"green\", attrs=['bold']))\n",
    "                model.load_state_dict(torch.load(model_save_path))\n",
    "            else:\n",
    "                print(colored(f\"\\nTraining beta-VAE with beta = {beta} and lambda = {lambda_reg}...\", \"red\", attrs=['bold']))\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "                criterion = lambda_beta_vae_loss\n",
    "\n",
    "                # Train the model and track the losses\n",
    "                train_loss_hist, val_loss_hist, kl_loss_hist, val_kl_loss_hist, recon_loss_hist, val_recon_loss_hist, l2_loss_hist, val_l2_loss_hist = train_lambda_beta_vae(\n",
    "                    model, train_loader, val_loader, optimizer, criterion, beta, lambda_reg, DEVICE, NUM_EPOCHS\n",
    "                )\n",
    "\n",
    "                # Save the loss histories\n",
    "                np.save(os.path.join(SAVE_PATH, f'train_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), train_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'kl_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), kl_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'bce_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), recon_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'l2_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), l2_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'val_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), val_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'val_kl_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), val_kl_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'val_bce_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), val_recon_loss_hist)\n",
    "                np.save(os.path.join(SAVE_PATH, f'val_l2_loss_hist_lambda_{lambda_reg}_beta_{beta}.npy'), val_l2_loss_hist)\n",
    "\n",
    "                # Save trained model\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                trained = True\n",
    "\n",
    "            # Extract latent vectors and ground truth factors for both train and validation datasets\n",
    "            print(colored(f\"\\nExtracting latent vectors and ground truth factors for beta = {beta} and lambda = {lambda_reg}...\", \"blue\", attrs=['bold']))\n",
    "            latent_vectors_train, latent_classes_train, latent_values_train, mu_list_train, std_list_train = extract_latents_and_factors(model, train_loader, DEVICE, 0)\n",
    "            latent_vectors_val, latent_classes_val, latent_values_val, mu_list_val, std_list_val = extract_latents_and_factors(model, val_loader, DEVICE, 1)\n",
    "\n",
    "            # Save latent vectors and factors for beta and lambda\n",
    "            np.save(os.path.join(SAVE_PATH, f'train_latent_vectors_lambda_{lambda_reg}_beta_{beta}.npy'), latent_vectors_train)\n",
    "            np.save(os.path.join(SAVE_PATH, f'train_latent_classes_lambda_{lambda_reg}_beta_{beta}.npy'), latent_classes_train)\n",
    "            np.save(os.path.join(SAVE_PATH, f'train_latent_values_lambda_{lambda_reg}_beta_{beta}.npy'), latent_values_train)\n",
    "            np.save(os.path.join(SAVE_PATH, f'train_mu_lambda_{lambda_reg}_beta_{beta}.npy'), mu_list_train)\n",
    "            np.save(os.path.join(SAVE_PATH, f'train_std_lambda_{lambda_reg}_beta_{beta}.npy'), std_list_train)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_latent_vectors_lambda_{lambda_reg}_beta_{beta}.npy'), latent_vectors_val)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_latent_classes_lambda_{lambda_reg}_beta_{beta}.npy'), latent_classes_val)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_latent_values_lambda_{lambda_reg}_beta_{beta}.npy'), latent_values_val)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_mu_lambda_{lambda_reg}_beta_{beta}.npy'), mu_list_val)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_std_lambda_{lambda_reg}_beta_{beta}.npy'), std_list_val)\n",
    "\n",
    "            # Plot training and validation losses\n",
    "            if trained:\n",
    "                print(colored(f\"\\nPlotting training and validation losses for beta = {beta} and lambda = {lambda_reg}...\", \"blue\", attrs=['bold']))\n",
    "                plot_losses(train_loss_hist, val_loss_hist, kl_loss_hist, val_kl_loss_hist, recon_loss_hist, val_recon_loss_hist, l2_loss_hist, val_l2_loss_hist, beta, lambda_reg)\n",
    "            \n",
    "            # Visualization functions\n",
    "            visualize_images_grid(model, val_loader, DEVICE, beta, BATCH_SIZE, lambda_reg)\n",
    "            visualize_random_decoder_outputs(model, DEVICE, LATENT_DIM, beta, lambda_reg)\n",
    "            gif_path = os.path.join(SAVE_PATH, f'latent_traversal_lambda_{lambda_reg}_beta_{beta}')\n",
    "            visualize_latent_traversal_combined(model, DEVICE, gif_path, distinct_shape_indices, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2698cdbd-46d4-4bdb-a4cd-27c3619511d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of square images: 36993\n",
      "Number of ellipse images: 36867\n",
      "Number of heart images: 36732\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Hyperparameters and constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NPZ_FILE = \"dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"\n",
    "SAVE_PATH = \"./lambda_beta_vae_results/\"\n",
    "LATENT_DIM = 10  # Dimensionality of the latent space\n",
    "BATCH_SIZE = 64  # Batch size for training and validation\n",
    "LEARNING_RATE = 1e-4  # Learning rate for the optimizer\n",
    "NUM_EPOCHS = 100  # Number of training epochs\n",
    "BETAS = [0.1, 0.5, 1, 2, 4, 6, 8]  # Different beta values for beta-VAE\n",
    "LAMBDAS = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Load dataset\n",
    "dataset = DspritesDataset(NPZ_FILE)\n",
    "\n",
    "# Ensure the results directory exists\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "# Define file paths for the indices\n",
    "train_indices_file = os.path.join(SAVE_PATH, \"train_indices.npy\")\n",
    "val_indices_file = os.path.join(SAVE_PATH, \"val_indices.npy\")\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Check if saved indices exist for a consistent split across runs\n",
    "if os.path.exists(train_indices_file) and os.path.exists(val_indices_file):\n",
    "    # Load saved indices using numpy\n",
    "    train_indices = np.load(train_indices_file)\n",
    "    val_indices = np.load(val_indices_file)\n",
    "else:\n",
    "    # Save the indices for reproducibility\n",
    "    train_indices, val_indices = torch.utils.data.random_split(\n",
    "        range(len(dataset)), [train_size, val_size], generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "\n",
    "    # Save indices using numpy\n",
    "    np.save(train_indices_file, train_indices)\n",
    "    np.save(val_indices_file, val_indices)\n",
    "\n",
    "# Create subsets using saved indices\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Select distinct shape indices (square, ellipse, heart) from the validation dataset\n",
    "distinct_shape_indices = select_3_distinct_shape_indices(val_dataset, list(range(val_size)))\n",
    "visualize_3_images_from_shapes(val_dataset, distinct_shape_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6092870-2fb3-4621-8a92-0ca7c58f9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
