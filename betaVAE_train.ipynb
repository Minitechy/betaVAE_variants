{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548f5c85-34f4-4bb2-a299-982a1bbae698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f002a265-8e02-49bf-a0e5-8ff67ada0c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Custom dataset class for dSprites dataset\n",
    "class DspritesDataset(Dataset):\n",
    "    def __init__(self, npz_file):\n",
    "        \"\"\"\n",
    "        Initialize the dSprites dataset by loading the image data and ground truth factors.\n",
    "\n",
    "        Args:\n",
    "            npz_file (str): Path to the .npz file containing the dataset.\n",
    "        \"\"\"\n",
    "        self.data = np.load(npz_file)\n",
    "        self.images = np.expand_dims(self.data['imgs'], axis=1)\n",
    "        self.latent_classes = self.data['latents_classes'][:, 1:]  # Skip the 'color' factor (always 1)\n",
    "        self.latent_values = self.data['latents_values'][:, 1:]  # Skip the 'color' factor (always 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of images.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch an image and its corresponding ground truth factors.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: Image as torch.Tensor, latent classes, and latent values.\n",
    "        \"\"\"\n",
    "        image = torch.tensor(self.images[idx], dtype=torch.float32)  # Convert image to float32 tensor\n",
    "        latent_classes = torch.tensor(self.latent_classes[idx], dtype=torch.int)  # Discrete factors\n",
    "        latent_values = torch.tensor(self.latent_values[idx], dtype=torch.float32)  # Continuous factors\n",
    "\n",
    "        return image, latent_classes, latent_values\n",
    "\n",
    "# Encoder architecture\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        Encoder for beta-VAE using convolutional layers.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Fully connected layers for mu and logvar\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)  # Fully connected layer with 256 units\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)  # Latent mean\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)  # Latent log-variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder to produce latent space parameters.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            mu (torch.Tensor): Mean of the latent space.\n",
    "            logvar (torch.Tensor): Log-variance of the latent space.\n",
    "        \"\"\"\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)  # Flatten the output of the convolution layers\n",
    "        h = self.fc1(h)  # Apply fully connected layer\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "# Decoder architecture\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_channels=1):\n",
    "        \"\"\"\n",
    "        Decoder for beta-VAE using transpose convolutional layers.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "            num_channels (int): Number of output image channels (1 for grayscale images).\n",
    "        \"\"\"\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(latent_dim, 256)  # Latent dim -> 256\n",
    "        self.fc2 = nn.Linear(256, 64 * 4 * 4)  # 256 -> 4x4x64\n",
    "        \n",
    "        # Transpose convolutional layers with adjusted padding\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, num_channels, kernel_size=4, stride=2, padding=1),  # 32x32 -> 64x64\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder to reconstruct the image from the latent space.\n",
    "\n",
    "        Args:\n",
    "            z (torch.Tensor): Latent vector.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed image.\n",
    "        \"\"\"\n",
    "        h = self.fc1(z)\n",
    "        h = self.fc2(h)\n",
    "        h = h.view(h.size(0), 64, 4, 4)  # Reshape to match convolutional layers\n",
    "        return self.decoder(h)\n",
    "\n",
    "# beta-VAE model\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        \"\"\"\n",
    "        beta-VAE model with encoder and decoder networks.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.encoder = ConvEncoder(latent_dim)\n",
    "        self.decoder = ConvDecoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from a Gaussian distribution.\n",
    "\n",
    "        Args:\n",
    "            mu (torch.Tensor): Mean of the latent space.\n",
    "            logvar (torch.Tensor): Log-variance of the latent space.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reparameterized latent vector.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)  # Random normal noise\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire VAE model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            recon_x (torch.Tensor): Reconstructed image.\n",
    "            mu (torch.Tensor): Mean of the latent space.\n",
    "            logvar (torch.Tensor): Log-variance of the latent space.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ce69ae-b1e4-4152-849b-fd3a43058a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_vae_loss(X_hat, X, Z_mu, Z_logvar, beta):\n",
    "    \"\"\"\n",
    "    Compute the total loss for beta-VAE.\n",
    "\n",
    "    Args:\n",
    "        X_hat (torch.Tensor): Reconstructed input (output of the decoder).\n",
    "        X (torch.Tensor): Original input.\n",
    "        Z_mu (torch.Tensor): Mean of the latent distribution.\n",
    "        Z_logvar (torch.Tensor): Log-variance of the latent distribution.\n",
    "        beta (float): Weight for the KL divergence term.\n",
    "\n",
    "    Returns:\n",
    "        bce_loss (torch.Tensor): Binary cross-entropy loss (summed over the batch).\n",
    "        kl_loss (torch.Tensor): KL divergence loss (summed over the batch).\n",
    "        total_loss (torch.Tensor): Total loss (bce_loss + beta * kl_loss).\n",
    "    \"\"\"\n",
    "    # Binary cross-entropy loss\n",
    "    bce_loss = F.binary_cross_entropy(X_hat, X, reduction='sum')\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + Z_logvar - Z_mu.pow(2) - Z_logvar.exp())\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = bce_loss + beta * kl_loss\n",
    "\n",
    "    return bce_loss, kl_loss, total_loss\n",
    "\n",
    "def train_beta_vae(model, train_loader, val_loader, optimizer, loss_fn, beta, device, num_epochs):\n",
    "    \"\"\"\n",
    "    Train the beta-VAE model.\n",
    "    \"\"\"\n",
    "    # Initialize loss histories\n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    kl_loss_hist, val_kl_loss_hist = [], []\n",
    "    bce_loss_hist, val_bce_loss_hist = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Step ---\n",
    "        model.train()\n",
    "        train_loss, kl_loss, bce_loss = 0, 0, 0\n",
    "\n",
    "        for batch, _, _ in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training'):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            X_hat, Z_mu, Z_logvar = model(batch)\n",
    "\n",
    "            # Compute loss\n",
    "            bce_loss_batch, kl_loss_batch, total_loss_batch = loss_fn(X_hat, batch, Z_mu, Z_logvar, beta)\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch losses\n",
    "            train_loss += total_loss_batch.item()\n",
    "            kl_loss += kl_loss_batch.item()\n",
    "            bce_loss += bce_loss_batch.item()\n",
    "\n",
    "        num_samples = len(train_loader.dataset)\n",
    "        train_loss_hist.append(train_loss / num_samples)\n",
    "        kl_loss_hist.append(kl_loss / num_samples)\n",
    "        bce_loss_hist.append(bce_loss / num_samples)\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        model.eval()\n",
    "        val_loss, val_kl_loss, val_bce_loss = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch, _, _ in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation'):\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                X_hat, Z_mu, Z_logvar = model(batch)\n",
    "\n",
    "                # Compute loss\n",
    "                bce_loss_batch, kl_loss_batch, total_loss_batch = loss_fn(X_hat, batch, Z_mu, Z_logvar, beta)\n",
    "\n",
    "                # Accumulate batch losses\n",
    "                val_loss += total_loss_batch.item()\n",
    "                val_kl_loss += kl_loss_batch.item()\n",
    "                val_bce_loss += bce_loss_batch.item()\n",
    "\n",
    "        num_samples = len(val_loader.dataset)\n",
    "        val_loss_hist.append(val_loss / num_samples)\n",
    "        val_kl_loss_hist.append(val_kl_loss / num_samples)\n",
    "        val_bce_loss_hist.append(val_bce_loss / num_samples)\n",
    "\n",
    "        # --- Logging the losses ---\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train BCE: {bce_loss_hist[-1]:.4f}, KL: {kl_loss_hist[-1]:.4f}, Total: {train_loss_hist[-1]:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Val BCE: {val_bce_loss_hist[-1]:.4f}, KL: {val_kl_loss_hist[-1]:.4f}, Total: {val_loss_hist[-1]:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_loss_hist,\n",
    "        'val_loss': val_loss_hist,\n",
    "        'kl_loss': kl_loss_hist,\n",
    "        'val_kl_loss': val_kl_loss_hist,\n",
    "        'bce_loss': bce_loss_hist,\n",
    "        'val_bce_loss': val_bce_loss_hist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e8bc48e-27af-4e36-8177-704f288dff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select exactly 3 distinct shape indices\n",
    "def select_3_distinct_shape_indices(dataset, split_indices):\n",
    "    \"\"\"\n",
    "    Select one index for each distinct shape (square, ellipse, heart).\n",
    "\n",
    "    Args:\n",
    "    - dataset: The dataset containing images and latent classes.\n",
    "    - split_indices: The indices corresponding to the data split (train, validation, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - distinct_shape_indices: A list containing one randomly selected index for each distinct shape.\n",
    "    \"\"\"\n",
    "    # Dictionary to store indices for each shape\n",
    "    shape_indices = {0: [], 1: [], 2: []}\n",
    "    \n",
    "    # Iterate over the split indices to collect indices for each shape\n",
    "    for idx in split_indices:\n",
    "        _, latent_classes, _ = dataset[idx]\n",
    "        shape_class = latent_classes[0].item()  # Shape class: 0 = square, 1 = ellipse, 2 = heart\n",
    "        \n",
    "        # Add the index to the corresponding shape list\n",
    "        shape_indices[shape_class].append(idx)\n",
    "    \n",
    "    # Print the number of images for each shape\n",
    "    shape_names = {0: 'square', 1: 'ellipse', 2: 'heart'}\n",
    "    for shape_class, indices in shape_indices.items():\n",
    "        print(f\"Number of {shape_names[shape_class]} images: {len(indices)}\")\n",
    "    \n",
    "    # Randomly select one index from each shape class\n",
    "    distinct_shape_indices = [random.choice(shape_indices[shape]) for shape in range(3)]\n",
    "    \n",
    "    return distinct_shape_indices\n",
    "\n",
    "def visualize_3_images_from_shapes(dataset, distinct_shape_indices):\n",
    "    \"\"\"\n",
    "    Visualize 3 images corresponding to distinct shapes (square, ellipse, heart) and save them.\n",
    "\n",
    "    Args:\n",
    "    - dataset: The dataset containing images and latent classes.\n",
    "    - distinct_shape_indices: A list of 3 indices corresponding to distinct shape images.\n",
    "    \"\"\"\n",
    "    # Ensure exactly 3 indices are provided\n",
    "    if len(distinct_shape_indices) != 3:\n",
    "        raise ValueError(\"distinct_shape_indices must contain exactly 3 indices.\")\n",
    "    \n",
    "    # Define the shape names\n",
    "    shape_names = ['square', 'ellipse', 'heart']\n",
    "    \n",
    "    # Create a figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    # Iterate through the indices, display, and save the images\n",
    "    for i, (idx, shape_name) in enumerate(zip(distinct_shape_indices, shape_names)):\n",
    "        image, _, _ = dataset[idx]  # Retrieve the image at the specified index\n",
    "        \n",
    "        # Display the image in the subplot\n",
    "        axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"{shape_name}\", fontsize=16)\n",
    "\n",
    "        # Save each image with its corresponding shape name\n",
    "        image_save_path = os.path.join(SAVE_PATH, f\"{shape_name}.png\")\n",
    "        plt.imsave(image_save_path, image.squeeze(), cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e708e3-e8e9-4d68-9623-f97c2cd42942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot losses for both training and validation over epochs\n",
    "def plot_beta_vae_losses(train_loss_hist, val_loss_hist, kl_loss_hist, val_kl_loss_hist, recon_loss_hist, val_recon_loss_hist, beta):\n",
    "    \"\"\"\n",
    "    Plot the total loss, KL divergence, and reconstruction error for both training and validation sets.\n",
    "\n",
    "    Args:\n",
    "    - train_loss_hist (list): History of training total losses (-ELBO).\n",
    "    - val_loss_hist (list): History of validation total losses (-ELBO).\n",
    "    - kl_loss_hist (list): History of training KL divergence losses.\n",
    "    - val_kl_loss_hist (list): History of validation KL divergence losses.\n",
    "    - recon_loss_hist (list): History of training reconstruction errors.\n",
    "    - val_recon_loss_hist (list): History of validation reconstruction errors.\n",
    "    - beta (float): The beta value used for the loss scaling in beta-VAE.\n",
    "    \"\"\"\n",
    "    epochs = len(train_loss_hist)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Subplot for Total Loss (-ELBO)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(range(epochs), train_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('Total Loss', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Subplot for KL Divergence\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(epochs), kl_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_kl_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('KL Divergence', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "    # Subplot for Reconstruction Error\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(epochs), recon_loss_hist, label=\"Training\", linestyle='-')\n",
    "    plt.plot(range(epochs), val_recon_loss_hist, label=\"Validation\", linestyle='--')\n",
    "    plt.xlabel('Epochs', fontsize=14)\n",
    "    plt.ylabel('BCE Loss', fontsize=14)\n",
    "    plt.xticks(ticks=[0, 20, 40, 60, 80, 100], fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(left=0.1, right=0.95, top=0.95, bottom=0.1, wspace=0.3)\n",
    "    plt.savefig(f\"{SAVE_PATH}beta_vae_loss_plots_{beta}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ccf43b6-e41a-4ea8-b741-15859fdf0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of original and reconstructed images in separate grids\n",
    "def visualize_images_grid(model, data_loader, device, beta, num_images=64):\n",
    "    \"\"\"\n",
    "    Visualize and save original and reconstructed images from a batch of data.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): Trained beta-VAE model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader for the dataset.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - beta (float): The beta value used for the loss scaling in beta-VAE.\n",
    "    - num_images (int): Number of images to visualize.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Retrieve one batch of images from the data loader\n",
    "    images, _, _ = next(iter(data_loader))\n",
    "    images = images[:num_images]\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        mu, logvar = model.encoder(images)\n",
    "        \n",
    "        # Sample z using the reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + std * epsilon\n",
    "\n",
    "        recon_images = model.decoder(z)\n",
    "\n",
    "    img_dim = images[0].cpu().squeeze().shape[0]\n",
    "\n",
    "    # Create separate images for original and reconstructed images\n",
    "    original_image_grid = np.ones((8 * img_dim, 8 * img_dim))  # 8 rows, 8 columns grid\n",
    "    reconstructed_image_grid = np.ones((8 * img_dim, 8 * img_dim))  # 8 rows, 8 columns grid\n",
    "\n",
    "    # Fill in the grids with images\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            idx = i * 8 + j\n",
    "            if idx < num_images:\n",
    "                # Fill the original image grid\n",
    "                original_image_grid[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim] = images[idx].cpu().squeeze()\n",
    "\n",
    "                # Fill the reconstructed image grid\n",
    "                reconstructed_image_grid[i * img_dim:(i + 1) * img_dim, j * img_dim:(j + 1) * img_dim] = recon_images[idx].cpu().squeeze()\n",
    "\n",
    "    # Save the original images only for the first beta = 1\n",
    "    if beta == 1:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(original_image_grid, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{SAVE_PATH}/original_images.png\", bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    # Save the reconstructed images for all betas\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(reconstructed_image_grid, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{SAVE_PATH}/reconstructed_images_beta_{beta}.png\", bbox_inches='tight', pad_inches=0)    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c24d781-61c5-4625-af82-63b7f91b680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_traversal_combined(model, device, gif_path, distinct_shape_indices, dataset, latent_dim=10, num_traversals=11):\n",
    "    \"\"\"\n",
    "    Visualize latent space traversal for dSprites dataset, creating both PNG and GIF outputs.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): Trained Beta-VAE model.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - gif_path (str): Path to save the generated GIF and PNG files.\n",
    "    - distinct_shape_indices (list): List of indices representing distinct shapes in the dataset.\n",
    "    - dataset (Dataset): dSprites dataset object.\n",
    "    - latent_dim (int): The number of latent dimensions (default=10).\n",
    "    - num_traversals (int): Number of traversal steps between -2 and 2 (default=11).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Define the interpolation range (from -2 to 2)\n",
    "    interpolation = torch.linspace(-2, 2, num_traversals)\n",
    "\n",
    "    # Prepare for storing frames for the final combined GIF (3 rows x num_traversals columns)\n",
    "    combined_gif_frames_all_shapes = [[] for _ in range(num_traversals)]\n",
    "\n",
    "    # Loop through each distinct shape (3 rows in the final GIF)\n",
    "    for row, idx in enumerate(distinct_shape_indices):\n",
    "        image, _, _ = dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "\n",
    "        mu, logvar = model.encoder(image)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std      \n",
    "\n",
    "        # Prepare for storing frames for the current shape's latent traversal\n",
    "        combined_gif_frames = [[] for _ in range(num_traversals)]\n",
    "\n",
    "        # Perform traversal for each latent dimension\n",
    "        for latent_idx in range(latent_dim):\n",
    "            z_ori = z.clone()  # Clone the latent vector\n",
    "\n",
    "            # Perform interpolation across the specified latent dimension\n",
    "            for i, alpha in enumerate(interpolation):\n",
    "                z_copy = z_ori.clone()\n",
    "                z_copy[:, latent_idx] = alpha  # Set this latent dimension to the interpolation value\n",
    "\n",
    "                # Generate the reconstructed image using the decoder\n",
    "                with torch.no_grad():\n",
    "                    generated_image = model.decoder(z_copy)\n",
    "\n",
    "                # Convert to numpy array and append the frame for saving\n",
    "                gen_img = generated_image.squeeze().cpu().numpy()\n",
    "                combined_gif_frames[i].append(np.uint8(255 * gen_img))\n",
    "\n",
    "        # Create grid for latent traversals (one shape, 10 latent dimensions, 11 traversal steps)\n",
    "        gif_frames_with_10_columns = [np.hstack(frame_list) for frame_list in combined_gif_frames]\n",
    "\n",
    "        # Store frames for all shapes into the combined GIF structure\n",
    "        for i in range(num_traversals):\n",
    "            if row == 0:\n",
    "                combined_gif_frames_all_shapes[i] = [gif_frames_with_10_columns[i]]\n",
    "            else:\n",
    "                combined_gif_frames_all_shapes[i].append(gif_frames_with_10_columns[i])\n",
    "\n",
    "        # Generate and save PNG for the current shape's latent traversals\n",
    "        png_output_path = f\"{gif_path}_image_{row + 1}.png\"\n",
    "        save_png_latent_traversal_only(combined_gif_frames, png_output_path, latent_dim, num_traversals)\n",
    "\n",
    "    # Combine frames of all shapes for the final GIF (3 rows x num_traversals columns)\n",
    "    final_gif_frames = [np.vstack(frame_list) for frame_list in combined_gif_frames_all_shapes]\n",
    "\n",
    "    # Save the final GIF combining all shapes and latent traversals\n",
    "    gif_output_path = f\"{gif_path}_combined.gif\"\n",
    "    save_combined_gif(final_gif_frames, gif_output_path)\n",
    "\n",
    "# Helper function to save the final combined GIF (3 rows and 10 columns) with column titles\n",
    "def save_combined_gif(frames, output_path):\n",
    "    \"\"\"\n",
    "    Save the combined GIF that shows the latent traversal for multiple shapes.\n",
    "\n",
    "    Args:\n",
    "    - frames (list): List of frames to include in the GIF.\n",
    "    - output_path (str): Path to save the GIF.\n",
    "    \"\"\"\n",
    "    gif_frames_with_labels = []\n",
    "\n",
    "    for frame in frames:\n",
    "        if frame is not None and frame.size > 0:\n",
    "            fig, ax = plt.subplots(figsize=(20, 6))\n",
    "            ax.imshow(frame, cmap='gray', vmin=0, vmax=255)  # Display the frame in grayscale\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add column labels (x_1 to x_10) for each latent dimension\n",
    "            column_labels = [f'$x_{{{i + 1}}}$' for i in range(10)]\n",
    "            for i, label in enumerate(column_labels):\n",
    "                x_pos = (frame.shape[1] / 10) * (i + 0.5)  # Calculate label position\n",
    "                ax.text(x_pos, -10, label, fontsize=14, ha='center', va='bottom', color='black')\n",
    "\n",
    "            # Ensure the figure is drawn before converting to an image\n",
    "            fig.canvas.draw()\n",
    "\n",
    "            # Convert the figure to an image array and append it to the list of frames\n",
    "            gif_frame = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8).reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "            gif_frames_with_labels.append(gif_frame)\n",
    "            plt.close(fig)\n",
    "\n",
    "    # Save the GIF\n",
    "    if gif_frames_with_labels:\n",
    "        # Convert images to uint8\n",
    "        gif_frames_with_labels = [np.uint8(frame) for frame in gif_frames_with_labels]\n",
    "        imageio.mimsave(output_path, gif_frames_with_labels, fps=5, loop=0)  # loop=0 for infinite looping\n",
    "    else:\n",
    "        print(\"No valid frames to save for GIF.\")\n",
    "\n",
    "# Function to save PNG for latent traversals with row labels for each latent dimension\n",
    "def save_png_latent_traversal_only(traversal_images, output_path, latent_dim=10, num_traversals=11):\n",
    "    \"\"\"\n",
    "    Save a PNG file for visualizing the latent traversals across all dimensions.\n",
    "\n",
    "    Args:\n",
    "    - traversal_images (list): List of images generated during the traversal.\n",
    "    - output_path (str): Path to save the PNG.\n",
    "    - latent_dim (int): Number of latent dimensions.\n",
    "    - num_traversals (int): Number of interpolation steps.\n",
    "    \"\"\"\n",
    "    # Create a figure with tight-fitting subplots\n",
    "    fig, axes = plt.subplots(latent_dim, num_traversals, figsize=(num_traversals * 2, latent_dim * 2))\n",
    "\n",
    "    # Define the interpolation values for the column labels from -2 to 2\n",
    "    interpolation_values = np.linspace(-2, 2, num_traversals)\n",
    "\n",
    "    # Plot the traversal images in a grid for all latent dimensions\n",
    "    for latent_idx in range(latent_dim):\n",
    "        for frame_idx in range(num_traversals):\n",
    "            axes[latent_idx, frame_idx].imshow(traversal_images[frame_idx][latent_idx], cmap='gray', aspect='auto')\n",
    "            axes[latent_idx, frame_idx].axis('off')\n",
    "\n",
    "    # Add row labels for each latent dimension (x_1 through x_10)\n",
    "    row_labels = [f'$x_{{{i + 1}}}$' for i in range(latent_dim)]\n",
    "    for i, label in enumerate(row_labels):\n",
    "        axes[i, 0].annotate(label, xy=(-0.05, 0.5), xycoords='axes fraction', xytext=(-30, 0), textcoords='offset points',\n",
    "                            ha='center', va='center', fontsize=18, color='black')\n",
    "\n",
    "    # Add column labels for x-axis (-2 to +2)\n",
    "    for frame_idx in range(num_traversals):\n",
    "        axes[0, frame_idx].set_title(f'{interpolation_values[frame_idx]:.1f}', fontsize=18, color='black')\n",
    "\n",
    "    plt.subplots_adjust(left=0.05, right=0.99, top=0.9, bottom=0.1, wspace=0, hspace=0)\n",
    "\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fced51a9-1ec3-4589-8116-fd97a24d67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract latent representations and ground truth factors\n",
    "def extract_latents_and_factors(model, data_loader, device, flag):\n",
    "    \"\"\"\n",
    "    Extract latent representations and ground truth factors.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): Trained beta-VAE model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader object to load the dataset.\n",
    "    - device (torch.device): Device to run the model on (CPU or GPU).\n",
    "    - flag (int): 0 for training data, 1 for validation data.\n",
    "    \n",
    "    Returns:\n",
    "    - latent_classes (numpy.ndarray): Discrete ground truth factors for the data.\n",
    "    - latent_values (numpy.ndarray): Continuous ground truth factors for the data.\n",
    "    - mu_list (numpy.ndarray): List of mean values for the latent variables.\n",
    "    - std_list (numpy.ndarray): List of standard deviations for the latent variables.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    latent_classes_list, latent_values_list = [], []\n",
    "    mu_list, std_list = [], []\n",
    "\n",
    "    desc = 'Training' if flag == 0 else 'Validation'\n",
    "    \n",
    "    # No gradient computation needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for image, latent_classes, latent_values in tqdm(data_loader, desc=desc):\n",
    "            image = image.to(device)\n",
    "            \n",
    "            mu, logvar = model.encoder(image)\n",
    "            sigma = torch.exp(0.5 * logvar)\n",
    "            \n",
    "            latent_classes_list.append(latent_classes)\n",
    "            latent_values_list.append(latent_values)\n",
    "            mu_list.append(mu.cpu())\n",
    "            std_list.append(sigma.cpu())\n",
    "\n",
    "    latent_classes = torch.cat(latent_classes_list, dim=0).numpy()\n",
    "    latent_values = torch.cat(latent_values_list, dim=0).numpy()\n",
    "    mu_list = torch.cat(mu_list, dim=0).numpy()\n",
    "    std_list = torch.cat(std_list, dim=0).numpy()\n",
    "    \n",
    "    return latent_classes, latent_values, mu_list, std_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5547077a-6667-474c-a625-ae1143baa216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for beta in BETAS:\n",
    "        print(colored(f\"\\nProcessing with beta = {beta}...\", \"green\", attrs=['bold']))\n",
    "            \n",
    "        # Define model and check if pre-trained weights exist\n",
    "        model_save_path = os.path.join(SAVE_PATH, f'beta_vae_{beta}.pth')\n",
    "        model = BetaVAE(latent_dim=LATENT_DIM).to(DEVICE)\n",
    "        trained = False  # Flag to indicate whether the model was trained\n",
    "\n",
    "        if os.path.exists(model_save_path):\n",
    "            print(colored(f\"Loading saved model weights for beta = {beta}...\", \"green\", attrs=['bold']))\n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "        else:\n",
    "            print(colored(f\"\\nTraining beta-VAE with beta = {beta}...\", \"red\", attrs=['bold']))\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = beta_vae_loss\n",
    "\n",
    "            # Train the model and track the losses\n",
    "            train_results = train_beta_vae(model, train_loader, val_loader, optimizer, criterion, beta, DEVICE, NUM_EPOCHS)\n",
    "\n",
    "            # Unpack the training results\n",
    "            train_loss_hist = train_results['train_loss']\n",
    "            val_loss_hist = train_results['val_loss']\n",
    "            kl_loss_hist = train_results['kl_loss']\n",
    "            val_kl_loss_hist = train_results['val_kl_loss']\n",
    "            bce_loss_hist = train_results['bce_loss']\n",
    "            val_bce_loss_hist = train_results['val_bce_loss']\n",
    "\n",
    "            np.save(os.path.join(SAVE_PATH, f'train_loss_hist_beta_{beta}.npy'), train_loss_hist)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_loss_hist_beta_{beta}.npy'), val_loss_hist)\n",
    "            np.save(os.path.join(SAVE_PATH, f'kl_loss_hist_beta_{beta}.npy'), kl_loss_hist)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_kl_loss_hist_beta_{beta}.npy'), val_kl_loss_hist)\n",
    "            np.save(os.path.join(SAVE_PATH, f'bce_loss_hist_beta_{beta}.npy'), bce_loss_hist)\n",
    "            np.save(os.path.join(SAVE_PATH, f'val_bce_loss_hist_beta_{beta}.npy'), val_bce_loss_hist)\n",
    "            \n",
    "            # Save trained model\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            trained = True\n",
    "\n",
    "        # Extract latent and ground truth factors for both training and validation datasets\n",
    "        print(colored(f\"\\nExtracting latent vectors and ground truth factors for beta = {beta}...\", \"blue\", attrs=['bold']))\n",
    "        latent_classes_train, latent_values_train, mu_list_train, std_list_train = extract_latents_and_factors(model, train_loader, DEVICE, 0)\n",
    "        latent_classes_val, latent_values_val, mu_list_val, std_list_val = extract_latents_and_factors(model, val_loader, DEVICE, 1)\n",
    "\n",
    "        np.save(os.path.join(SAVE_PATH, f'train_latent_classes_beta_{beta}.npy'), latent_classes_train)\n",
    "        np.save(os.path.join(SAVE_PATH, f'train_latent_values_beta_{beta}.npy'), latent_values_train)\n",
    "        np.save(os.path.join(SAVE_PATH, f'train_mu_beta_{beta}.npy'), mu_list_train)\n",
    "        np.save(os.path.join(SAVE_PATH, f'train_std_beta_{beta}.npy'), std_list_train)\n",
    "        np.save(os.path.join(SAVE_PATH, f'val_latent_classes_beta_{beta}.npy'), latent_classes_val)\n",
    "        np.save(os.path.join(SAVE_PATH, f'val_latent_values_beta_{beta}.npy'), latent_values_val)\n",
    "        np.save(os.path.join(SAVE_PATH, f'val_mu_beta_{beta}.npy'), mu_list_val)\n",
    "        np.save(os.path.join(SAVE_PATH, f'val_std_beta_{beta}.npy'), std_list_val)\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        if trained:\n",
    "            print(colored(f\"\\nPlotting training and validation losses for beta = {beta}...\", \"blue\", attrs=['bold']))\n",
    "            plot_beta_vae_losses(train_loss_hist, val_loss_hist, kl_loss_hist, val_kl_loss_hist, bce_loss_hist, val_bce_loss_hist, beta)\n",
    "\n",
    "        # Visualization functions\n",
    "        visualize_images_grid(model, val_loader, DEVICE, beta)\n",
    "\n",
    "        gif_path = os.path.join(SAVE_PATH, f'latent_traversal_beta_{beta}')\n",
    "        visualize_latent_traversal_combined(model, DEVICE, gif_path, distinct_shape_indices, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662e3951-e239-4b4a-9f60-7ec3fcaf33a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A5000\n",
      "GPU 1: NVIDIA GeForce RTX 2080 Ti\n",
      "Using GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU found.\")\n",
    "\n",
    "# Set GPU 1 as the default device\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2698cdbd-46d4-4bdb-a4cd-27c3619511d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of square images: 24736\n",
      "Number of ellipse images: 24658\n",
      "Number of heart images: 24334\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Hyperparameters and constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NPZ_FILE = \"dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\"\n",
    "SAVE_PATH = \"./betaVAE_results/\"\n",
    "LATENT_DIM = 10  # Dimensionality of the latent space\n",
    "BATCH_SIZE = 64  # Batch size for training and validation\n",
    "LEARNING_RATE = 1e-4  # Learning rate for the optimizer\n",
    "NUM_EPOCHS = 100  # Number of training epochs\n",
    "BETAS = [0.1, 1, 2, 4, 8, 16]  # Different beta values for beta-VAE\n",
    "\n",
    "# Load dataset\n",
    "dataset = DspritesDataset(NPZ_FILE)\n",
    "\n",
    "# Ensure the results directory exists\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "# Define file paths for the indices\n",
    "train_indices_file = os.path.join(SAVE_PATH, \"train_indices.npy\")\n",
    "val_indices_file = os.path.join(SAVE_PATH, \"val_indices.npy\")\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Check if saved indices exist for a consistent split across runs\n",
    "if os.path.exists(train_indices_file) and os.path.exists(val_indices_file):\n",
    "    # Load saved indices using numpy\n",
    "    train_indices = np.load(train_indices_file)\n",
    "    val_indices = np.load(val_indices_file)\n",
    "else:\n",
    "    # Save the indices for reproducibility\n",
    "    train_indices, val_indices = torch.utils.data.random_split(\n",
    "        range(len(dataset)), [train_size, val_size], generator=torch.Generator().manual_seed(SEED)\n",
    "    )\n",
    "\n",
    "    # Save indices using numpy\n",
    "    np.save(train_indices_file, train_indices)\n",
    "    np.save(val_indices_file, val_indices)\n",
    "\n",
    "# Create subsets using saved indices\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Select distinct shape indices (square, ellipse, heart) from the validation dataset\n",
    "distinct_shape_indices = select_3_distinct_shape_indices(val_dataset, list(range(val_size)))\n",
    "visualize_3_images_from_shapes(val_dataset, distinct_shape_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5dd559-9c63-43cd-a5bb-0d8c50181661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      "Processing with beta = 0.1...\u001b[0m\n",
      "\u001b[1m\u001b[31m\n",
      "Training beta-VAE with beta = 0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 1/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 369.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train BCE: 116.5152, KL: 47.6044, Total: 121.2756\n",
      "Epoch [1/100] Val BCE: 50.5441, KL: 40.8016, Total: 54.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 2/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 365.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Train BCE: 37.4183, KL: 43.0732, Total: 41.7256\n",
      "Epoch [2/100] Val BCE: 31.0451, KL: 43.4791, Total: 35.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 3/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 360.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Train BCE: 28.9007, KL: 44.0124, Total: 33.3020\n",
      "Epoch [3/100] Val BCE: 26.9959, KL: 43.8934, Total: 31.3853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 4/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 367.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Train BCE: 25.8772, KL: 44.2429, Total: 30.3015\n",
      "Epoch [4/100] Val BCE: 24.8093, KL: 44.1584, Total: 29.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 5/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 353.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Train BCE: 23.7299, KL: 44.5157, Total: 28.1815\n",
      "Epoch [5/100] Val BCE: 22.5559, KL: 44.7839, Total: 27.0342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 6/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 358.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Train BCE: 21.9534, KL: 44.8010, Total: 26.4335\n",
      "Epoch [6/100] Val BCE: 20.9925, KL: 45.0087, Total: 25.4934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 7/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 352.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Train BCE: 20.5995, KL: 45.0344, Total: 25.1029\n",
      "Epoch [7/100] Val BCE: 19.7606, KL: 44.8997, Total: 24.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 8/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 352.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Train BCE: 19.5189, KL: 45.1715, Total: 24.0360\n",
      "Epoch [8/100] Val BCE: 18.8056, KL: 45.1800, Total: 23.3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 9/100 - Validation: 100%|████████████| 1152/1152 [00:03<00:00, 350.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Train BCE: 18.6479, KL: 45.2285, Total: 23.1707\n",
      "Epoch [9/100] Val BCE: 18.0593, KL: 45.0141, Total: 22.5607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "poch 10/100 - Validation: 100%|███████████| 1152/1152 [00:03<00:00, 356.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Train BCE: 17.9083, KL: 45.2712, Total: 22.4354\n",
      "Epoch [10/100] Val BCE: 17.3269, KL: 45.1819, Total: 21.8451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Training:  86%|███████████▏ | 8940/10368 [01:51<00:18, 78.72it/s]"
     ]
    }
   ],
   "source": [
    "# Main script execution\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
